{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Wrangle and Analyze WeRateDogs Tweet \n",
    "*<span style=\"font-family:Arial; font-size:1.3em;\">By: Redi Sunarta - Udacity Student (2020)</span>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Report\n",
    "---\n",
    "### Gathering Data\n",
    "I gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then cleaning those data. The dataset to be used is the tweet archive of Twitter user [WeRateDogs](https://twitter.com/dog_rates). WeRateDogs downloaded their Twitter archive and sent it to Udacity via email exclusively for you to use in this project as my `data1`. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017 or more on this soon. Second dataset, as my `data2`, is the tweet image predictions is present in each tweet according to a neural network. This file is hosted on Udacity's servers and should be downloaded programmatically. However, these two dataset didnâ€™t contain a information about the likes counts nor retweet counts of each tweet. Therefore, I had to be gathered through a twitter API, as my `data3`, using the tweepy package one by one tweet to ensure meet specifications in the Project Rubric.\n",
    "### Assessing Data Quality and Tidiness\n",
    "After gather process done, I load each dataset in oder to assessing their qualities and tidiness. I implemented two assessment methods that have been taught, visual and programmatic way. Even though, it's hard to find the data problem with visual assessment. Hence, almost all assessment results are found by programatic assessment. I observed in, at least,  total five issues and ten issues respectively regarding tidiness and quality of the datasets. As follow:\n",
    "#### Quality\n",
    "- tweet_id datatype in `data1` and `data2` is int (shoud be object type). This will will be problem if we merge with `data3`.\n",
    "- Data type of column timestamp (created_at) in `data1` (`data3`) is still object type. Those columns should be a datetime type.\n",
    "- Dog name in `data1` with value 'None', 'a', and 'the' most likely is missing values. In other word, those values refer to the same thing.\n",
    "- **The removal of retweets and replies**. There are 78 replies and 181 retweets tweet type, I shoud drop them.\n",
    "- There are 217 rows of expanded_url in `data1` may be not WeRateDogs own tweet which outside our objective. We must exclue them with assumption that WeRateDogs didn't chage in the period.\n",
    "- Large missing values (NaN) in five `data1` column, example in_reply_to_status_id and in_reply_to_user_id.\n",
    "- Data type of columns, doggo, floofer, pupper, puppo in `data1` should be binary variable; None value as False, otherwise is True. However, 'None' value also can refer to missing values, we should check first.\n",
    "- Continuation of the last point, 13 row/tweet have more than one dog stage. I'll drop it in order to ease analysis.\n",
    "- For common people, the name of columns not descriptive name: p1, p1_conf, p1_dog, etc. Nondescriptive column headers.\n",
    "- WeRateDogs twitter account have millions follower. So, almost impossible they tweet have zero like_count or retweet_count in `data3`. I was check several of them, yap thats a error, I should convert them to NaN.\n",
    "- (Additional) p1, p2, p3 value could be deleted underscore and capitalize the name.\n",
    "                                                                  \n",
    "#### Tidiness\n",
    "- The text column in `data1` contains value for column name, link, rating_numerator, and rating_denominator.\n",
    "- Moreover, rating variable could be just one column in `data1`, rating_numerator devide by rating_denominator.\n",
    "- The columns respectively, doggo, floofer, pupper, puppo in `data1` could be catergorical variable.\n",
    "- The table contain same observations (tweet_id), thus could be only one table. Thus, we can merge them up.\n",
    "- However, we early assume that each tweet_id contain different dog. Yet, if we can prove that any same dog in different tweet, then it will be appropriate to make new table.\n",
    "\n",
    "### Data Cleaning\n",
    "Based on those assesment finding, I cleaned each point even though it will not perfectly clean. Most of them I cleaned use for looping combine with pandas.series.apply method, if those columns have similiar issues. In summarize, I utilize pandas fuction as my cleaning tools. Then, last step cleaning proccess, I started by merging, inner join,  the three different data sets with `tweet_id` as primary key into one master data frame, while fixing remain tidiness issues by the way, like drop the useless columns.\n",
    "### Storing Data\n",
    "Finally, data wrangling process is considered complete. I stored the cleaned dataset in csv file, `twitter_archive_master.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
